import torch
import tensorflow as tf
from torch import nn
from torch.autograd import Variable
import torch.utils.data as Data
import numpy as np
import torch.nn.functional as F
import data_helpers
from tensorflow.contrib import learn
# h.p. define
torch.manual_seed(1)
EPOCH = 100
BATCH_SIZE = 64
LR = 0.01
HIDDEN_NUM = 128
HIDDEN_LAYER = 2
# process data
print("Loading data...")
x_text1, y1 = data_helpers.load_data('Training.xml')
x_text2, y2 = data_helpers.load_data_and_labels('./data/train.csv')
x_text3, y3 = data_helpers.load_data_('Test.xml')
x_text = np.concatenate([x_text1, x_text2, x_text3])
y = np.concatenate([y1, y2, y3])
# Build vocabulary
max_document_length = max([len(x.split(" ")) for x in x_text])
length_text = [len(item) for item in list(x_text)]
length_text = np.array(length_text)
print('max sentence length', max_document_length)
vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)
print(x_text)
x = list(vocab_processor.fit_transform(x_text))
'''
#   padding zero from position 0
for i in range(281):
    zeropos = list(x[i]).index(0)
    for j in range(1, zeropos+1):
        x[i][-j], x[i][zeropos-j] = x[i][zeropos-j], x[i][-j]
        #   print('exchange: ', zeropos-j, -j)
'''
x = np.array(x)
#   print(x.shape)

# Randomly shuffle data
np.random.seed(10)
shuffle_indices = np.random.permutation(np.arange(len(y)))
print(shuffle_indices.shape)
print('x shape ', x.shape)

x_shuffled = x[shuffle_indices]
y_shuffled = y[shuffle_indices]
length_shuffled = length_text[shuffle_indices]
print('line60', len(x_shuffled))
# Split train/test set
# TODO: This is very crude, should use cross-validation
dev_sample_index = -1 * int(0.1 * float(len(y)))
x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]
y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]
length_train, length_dev = length_shuffled[:dev_sample_index], length_shuffled[dev_sample_index:]
print("Vocabulary Size: {:d}".format(len(vocab_processor.vocabulary_)))
print("Train/Dev split: {:d}/{:d}".format(len(y_train), len(y_dev)))
x_train = torch.from_numpy(x_train)
#   x_train = torch.unsqueeze(x_train, dim=2)
y_train = torch.from_numpy(y_train).long()
x_dev = Variable(torch.from_numpy(x_dev))
y_dev = Variable(torch.max(torch.from_numpy(y_dev).long(), dim=1)[1])


torch_dataset = Data.TensorDataset(data_tensor=x_train, target_tensor=y_train)
loader = Data.DataLoader(
    dataset=torch_dataset,
    batch_size=128,
    shuffle=True
)


class LSTM(nn.Module):
    def __init__(self):
        super(LSTM, self).__init__()
        self.emdedding = nn.Embedding(len(vocab_processor.vocabulary_), 64)
        self.lstm = nn.LSTM(
            input_size=64,  # dim of word vector
            hidden_size=30,  # dim of output of lstm nn
            num_layers=2,  # num of layers
            batch_first=True,  # batck first
            dropout=0.5,
        )
        self.out = nn.Linear(30, 12)  # output for emotion
        self.out2 = nn.Linear(12, 7)

    def forward(self, x):
        x = self.emdedding(x)
        lstm_out, (h_n, h_c) = self.lstm(x, None)
        out = F.relu(lstm_out)
        out = self.out(lstm_out[:, -1, :])
        out = F.relu(out)
        out2 = self.out2(out)
        out2 = F.softmax(out2)
        return out2


model = LSTM()
model.train()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
loss_func = nn.CrossEntropyLoss()
print(model)


def get_test():
    model.eval()
    print('start dev test')
    test_output = model(x_dev)
    print(test_output)
    input("please ensure the output")
    test_output = torch.max(test_output, dim=1)[1]
    right = 0
    for i in range(0, 2117):
        if test_output[i].data.numpy() == y_dev[i].data.numpy():
            right += 1
    print('test set accuracy: ', float(right / 2117.0) * 100, '%')
    #   torch.save(model, str(epoch)+'_model.pkl')
    model.train()


for epoch in range(EPOCH):
    for index, (batch_x, batch_y) in enumerate(loader):
        batch_x = Variable(batch_x)
    #   one hot to scalar
        batch_y = torch.max(batch_y, dim=1)[1]
        batch_y = torch.squeeze(batch_y)
        batch_y = Variable(batch_y)
        output = model(batch_x)
    #   print(batch_y)
    #   print(output)
    #   input("waiting for your ensuring")
        loss = loss_func(output, batch_y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()


    #   get accuracy
        output = torch.squeeze(torch.torch.max(output, dim=1)[1])
        right = 0
        if index % 20 == 0:
            get_test()
        try:
            for i in range(0, 128):
                if output[i].data.numpy() == batch_y[i].data.numpy():
                    right += 1
            print('accuracy: ', 100.0*float(right/128.0), '%')
        except:
            print('fatal error occurs')

    print('epoch: ', epoch, 'has been finish')

